{"version":3,"sources":["utils/inference.js","components/InferenceResults.js","components/InferenceRow.js","components/InferenceShowcase.js","components/ModelShowcase.js","models/MNIST.js","models/MobileNet_labels.js","models/MobileNet.js","App.js","serviceWorker.js","index.js"],"names":["softmax","arr","C","Math","max","d","map","y","exp","reduce","a","b","value","infer","model","session","tensor","start","Date","run","outputData","end","time","getTime","output","values","next","postprocess","data","probabilities","prediction","Column","Table","InferenceResults","props","top_n","probs","orderBy","prob","key","label","slice","dataSource","className","pagination","title","dataIndex","render","probability","span","min","step","toFixed","style","width","InferenceRow","initialInfResult","loading","useState","inferenceResult","setInferenceResult","imgSize","canvasElement","useRef","collapsed","setCollapsed","crop","loadImage","picture","base64data","maxWidth","canvas","cover","blueimg","current","console","warn","ctx","getContext","log","drawImage","image","height","img","getImageData","result","setTimeout","useEffect","drawimg","RemoveButton","onClick","onRemove","type","icon","CloseCircleOutlined","InferenceButton","canInfere","tooltip","inferimg","disabled","color","CollapseButton","RightOutlined","DownOutlined","Item","actions","Meta","file","name","replace","description","avatar","ref","minWidth","margin","InferenceShowcase","pictures","setPictures","imageUploader","loadPictureFromUrl","url","fetch","then","response","blob","Promise","resolve","reject","filename","split","pop","File","startsWith","reader","FileReader","onloadend","onerror","readAsDataURL","pictureUrls","all","renderItem","removeImage","base64","display","onChange","files","pics","i","ModelShowcase","msg","success","feedback","state","setState","InferenceSession","backendHint","loadModel","modelFile","res","message","failure","background","textAlign","status","subTitle","children","child","React","cloneElement","imgdata","input","Float32Array","len","length","Tensor","outputdata","Array","prototype","call","argmax","n","mean","std","indexOf","labels","Text","Typography","Link","Paragraph","refs","href","text","short","Ref","keyword","reference","find","toLowerCase","includes","App","p","process","code","rel","dateTime","src","alt","language","docco","MNIST","border","role","aria-label","MobileNet","id","Boolean","window","location","hostname","match","ReactDOM","StrictMode","document","getElementById","navigator","serviceWorker","ready","registration","unregister","catch","error"],"mappings":"qfACO,SAASA,EAAQC,GACpB,IAAMC,EAAIC,KAAKC,IAAL,MAAAD,KAAI,YAAQF,IAChBI,EAAIJ,EAAIK,KAAI,SAACC,GAAD,OAAOJ,KAAKK,IAAID,EAAIL,MAAIO,QAAO,SAACC,EAAGC,GAAJ,OAAUD,EAAIC,KAC/D,OAAOV,EAAIK,KAAI,SAAAM,GACX,OAAOT,KAAKK,IAAII,EAAQV,GAAKG,KAU9B,SAAeQ,EAAtB,sC,4CAAO,WAAqBC,EAAOC,EAASC,GAArC,6BAAAN,EAAA,6DACGO,EAAQ,IAAIC,KADf,SAEsBH,EAAQI,IAAI,CAAEH,IAFpC,cAEGI,EAFH,OAGGC,EAAM,IAAIH,KACVI,EAAQD,EAAIE,UAAYN,EAAMM,UAC9BC,EAASJ,EAAWK,SAASC,OAAOd,MALvC,EAMmCE,EAAMa,YAAYH,EAAOI,MAAvDC,EANL,EAMKA,cAAeC,EANpB,EAMoBA,WANpB,kBAOI,CAAER,OAAMO,gBAAeC,eAP3B,4C,gFCZCC,EAAWC,IAAXD,OAED,SAASE,EAAiBC,GAAQ,IAC/BL,EAA8BK,EAA9BL,cAAeC,EAAeI,EAAfJ,WACjBK,EAAQD,EAAMC,OAAS,GAGvBC,EADUC,IAAQR,EAAe,CAAC,eAAgB,CAAC,SACnCvB,KAAI,SAAAgC,GAAI,oBAC5BC,IAAKD,EAAKE,OACPF,MACDG,MAAM,EAAGN,GACb,OACE,kBAAC,IAAD,CAAOO,WAAYN,EAAOO,UAAU,oBAClCC,YAAY,GACZ,kBAACb,EAAD,CAAQc,MAAM,QAAQC,UAAU,QAAQP,IAAI,QAAQQ,OAAQ,SAAAP,GAAK,OAC/D,oCAAGA,IAAUV,EAAa,2BAAIU,GAAa,8BAAOA,OAEpD,kBAACT,EAAD,CAAQc,MAAM,cAAcC,UAAU,cAAcP,IAAI,cACtDQ,OAAQ,SAAAC,GAAW,OACjB,kBAAC,IAAD,KACE,kBAAC,IAAD,CAAKC,KAAM,IACT,kBAAC,IAAD,CAAQC,IAAK,EAAG9C,IAAK,EAAG+C,KAAM,IAC5BvC,MAAOoC,EAAYI,QAAQ,MAC/B,kBAAC,IAAD,CAAKH,KAAM,GACT,kBAAC,IAAD,CAAaC,IAAK,EAAG9C,IAAK,EAAG+C,KAAM,IACjCvC,MAAOoC,EAAYI,QAAQ,GAAIC,MAAO,CAAEC,MAAO,gBCrBxD,SAASC,EAAarB,GAE3B,IAAMsB,EAAmB,CACvBlC,MAAO,EACPO,cAAe,GACfC,WAAY,KACZ2B,SAAS,GANuB,EAQYC,mBAASF,GARrB,mBAQ3BG,EAR2B,KAQVC,EARU,KAS1BC,EAAY3B,EAAMpB,MAAlB+C,QACFC,EAAgBC,iBAAO,MAVK,EAWAL,oBAAS,GAXT,mBAW3BM,EAX2B,KAWhBC,EAXgB,iDAclC,WAAuBC,GAAvB,iBAAAxD,EAAA,sEACwByD,IAAUjC,EAAMkC,QAAQC,WAAY,CACxDC,SAAUpC,EAAMpB,MAAM+C,QACtBK,MAAM,EACNK,QAAQ,EACRC,OAAO,IALX,UACQC,EADR,OAOOX,EAAcY,QAPrB,yCAOqCC,QAAQC,KAAK,wBAPlD,OAQQC,EAAMf,EAAcY,QAAQI,WAAW,MAC1CZ,GAEDS,QAAQI,IAAI,SACZF,EAAIG,UAAUP,EAAQQ,OAAQ,IAAK,GAAI,IAAK,MAE5CJ,EAAIG,UAAUP,EAAQQ,MAAO,EAAG,GAdpC,4CAdkC,kEAiClC,sCAAAvE,EAAA,yDACEkD,EAAmB,eAAKJ,EAAN,CAAwBC,SAAS,KAC3C1C,EAAmBmB,EAAnBnB,QAASD,EAAUoB,EAAVpB,MAGZgD,EAAcY,QALrB,yCAKqCC,QAAQC,KAAK,yBALlD,cAMQC,EAAMf,EAAcY,QAAQI,WAAW,MAC7CH,QAAQI,IAAIF,EAAIN,OAAOjB,MAAOuB,EAAIN,OAAOW,QACnCC,EAAMN,EAAIO,aAAa,EAAG,EAAGP,EAAIN,OAAOjB,MAAOuB,EAAIN,OAAOW,QAE1DlE,EAASF,EAAME,OAAOmE,EAAKN,GAVnC,UAWuBhE,EAAMC,EAAOC,EAASC,GAX7C,QAWQqE,EAXR,OAYEV,QAAQI,IAAI,mBAAoBM,GAGhCC,YAAW,WACT1B,EAAmB,eAAKyB,EAAN,CAAc5B,SAAS,OACxC,KAjBL,6CAjCkC,sBAqDlC8B,qBAAU,WACHrD,EAAMkC,QAAQC,YAtDa,oCAwDhCmB,CAAQtD,EAAMgC,QAEb,CAAChC,EAAMkC,QAAQC,WAAYnC,EAAMpB,MAAM+C,QAAS3B,EAAMnB,UAEzD,IAAM0E,EAAe,kBACnB,kBAAC,IAAD,CAAS5C,MAAM,kBACb,kBAAC,IAAD,CAAQ6C,QAAS,kBAAMxD,EAAMyD,YAAYC,KAAK,OAC5CC,KAAM,kBAACC,EAAA,EAAD,UAIJrC,EAA6CE,EAA7CF,QAASnC,EAAoCqC,EAApCrC,KAAMO,EAA8B8B,EAA9B9B,cAAeC,EAAe6B,EAAf7B,WAChCiE,EAAkB,WACtB,IAAMC,GAAa9D,EAAMnB,UAAYmB,EAAMkC,QAAQC,WAC/C4B,EAAU,oBAGd,OAFK/D,EAAMnB,UAASkF,EAAU,8BACzB/D,EAAMkC,QAAQC,aAAY4B,EAAU,mBAEvC,oCACE,kBAAC,IAAD,KACE,kBAAC,IAAD,CAASpD,MAAOoD,GACd,kBAAC,IAAD,CAAQP,QAAS,kBA7EO,0CA6EDQ,IAAYzC,QAASA,EAC1C0C,SAAUH,GADZ,eAMJ,kBAAC,IAAD,KACE,2BAAO3C,MAAO,CAAE+C,MAAO,UACV,IAAV9E,EAAA,yBAAgCA,EAAhC,MAA2C,gDAOhD+E,EAAiB,WACrB,OAAIrC,EACK,kBAAC,IAAD,CAAQ0B,QAAS,kBAAMzB,GAAa,IAAQ2B,KAAK,OACtDC,KAAM,kBAACS,EAAA,EAAD,QAED,kBAAC,IAAD,CAAQZ,QAAS,kBAAMzB,GAAa,IAAO2B,KAAK,OACrDC,KAAM,kBAACU,EAAA,EAAD,SAGZ,OACE,kBAAC,IAAKC,KAAN,CAAWC,QAAS,CAAC,kBAAChB,EAAD,MAAkB,kBAACM,EAAD,OACrCpD,UAAU,eACV,kBAAC,IAAK6D,KAAKE,KAAX,CAAgB7D,MAAOX,EAAMkC,QAAQuC,KAAKC,KAAKC,QAAQ,IAAK,KAC1DC,YAAW,UAAKjD,EAAL,cAAkBA,GAC7BkD,OAAQ7E,EAAMkC,QAAQC,WACpB,4BAAQ2C,IAAKlD,EAAeR,MAAOO,EAASqB,OAAQrB,EACpDR,MAAO,CAAE4D,SAAU,GAAI3C,SAAU,OACjC,kBAAC,IAAD,CAAOwC,YAAY,4BACjBzD,MAAO,CAAE6D,OAAQ,YAGvB,yBAAKvE,UAAU,0BACb,kBAAC0D,EAAD,OAEF,kBAACpE,EAAD,CAAkBJ,cAAeA,EAAeC,WAAYA,EAC1DK,MAAO6B,EAAY,EAAI,MC3ChBmD,MA5Ef,SAA2BjF,GAAQ,IAAD,EACAwB,mBAAS,IADT,mBACzB0D,EADyB,KACfC,EADe,KAE1BC,EAAgBvD,iBAAO,MAIvBwD,EAAqB,SAAAC,GAAG,OAAIC,MAAMD,GACrCE,MAAK,SAAAC,GAAQ,OAAIA,EAASC,UAC1BF,MAAK,SAAAE,GAAI,OAAI,IAAIC,SAAQ,SAACC,EAASC,GAAY,IACtCnC,EAASgC,EAAThC,KACFoC,EAAWR,EAAIS,MAAM,KAAKC,MAC1BvB,EAAO,IAAIwB,KAAK,CAACP,GAAOI,EAAU,CAAEpC,SAIrCA,EAAKwC,WAAW,WACnBzD,QAAQC,KAAK,kCAA4B+B,EAAKC,KAAjC,0BACGY,EADH,OAEbM,EAAQ,CAAEnB,OAAMtC,WAAY,QAI9B,IAAMgE,EAAS,IAAIC,WACnBD,EAAOE,UAAY,kBAAMT,EAAQ,CAC/BnB,OACAtC,WAAYgE,EAAOhD,UAErBgD,EAAOG,QAAUT,EACjBM,EAAOI,cAAcb,UAyBzB,OArBArC,qBAAU,WACHrD,EAAMwG,aACXb,QAAQc,IAAIzG,EAAMwG,YAAYpI,IAAIiH,IAC/BG,KAAKL,KACP,CAACnF,EAAMwG,cAkBR,6BACE,kBAAC,IAAD,CAAM/F,UAAU,cAAcD,WAAY0E,EACtCwB,WAAY,SAAAxE,GAAO,OACnB,kBAACb,EAAD,CAAca,QAASA,EAASuB,SAAU,kBARjC,SAAAvB,GACfkD,EAAc5C,QAAQmE,YAAYzE,EAAQ0E,QAOYnD,CAASvB,IACrDrD,QAASmB,EAAMnB,QACfD,MAAOoB,EAAMpB,MACboD,KAAMhC,EAAMgC,UAGpB,yBAAKvB,UAAU,gBAIbU,MAAO,CACL0F,QAAS7G,EAAMwG,YAAc,OAAS,WAExC,kBAAC,IAAD,CAAeM,SA/BJ,SAACC,EAAO7B,GAEvB,IAAM8B,EAAOD,EAAM3I,KAAI,SAACqG,EAAMwC,GAAP,MAAc,CACnCxC,OAAMtC,WAAY+C,EAAS+B,OAG7B9B,EAAY6B,IAyB2BlC,IAAKM,OCGjC8B,MAzEf,SAAuBlH,GAAQ,IAAD,EACFwB,mBAAS,CACjC2F,IAAK,WAAY5F,SAAS,EAAO6F,SAAS,EAAOvI,QAAS,KAC1DwI,SAAU,+CAHgB,mBACrBC,EADqB,KACdC,EADc,KAO5BlE,qBAAU,WACR,GAAKiE,EAAM/F,QAAX,CACA,IAAM1C,EAAU,IAAI2I,mBAAiB,CAAEC,YAAa,UACpD5I,EAAQ6I,UAAU1H,EAAM2H,WAAWnC,MAAK,WACtC/C,QAAQI,IAAI,8BAGZO,YAAW,WACTmE,EAAS,CACPJ,IAAI,4BACJE,SAAU,wCAEVD,SAAS,EACTvI,cAED,QACF,SAAA+I,GACDL,EAAS,CACPJ,IAAK,kCACLE,SAAUO,EAAIC,QACdtG,SAAS,EACTuG,SAAS,IAEXrF,QAAQC,KAAK,uBAAwBkF,SAEtC,CAAC5H,EAAM2H,UAAWL,EAAM/F,UAhCC,IAkCpBoG,EAAc3H,EAAd2H,UACF7B,EAAW6B,GAAaA,EAAUhD,QAAQ,WAAY,IAE5D,OACE,yBAAKxD,MAAO,CAAE4G,WAAY,QAAS/C,OAAQ,WACzC,yBAAK7D,MAAO,CAAC6G,UAAW,WACtB,yBAAK7G,MAAO,CAAC6D,OAAQ,OAAQ6B,QAAS,WAAYf,GAClD,kBAAC,IAAD,CAAQtC,QAAS,kBAAM+D,EAAS,CAC9BJ,IAAK,aACL5F,SAAS,EACT6F,SAAS,KACPnD,SAAUqD,EAAM/F,SAJpB,cAOA,kBAAC,IAAD,CACE0G,OAAQX,EAAMF,QAAU,UACrBE,EAAMQ,QAAU,QAAU,OAC7BnH,MAAO2G,EAAMH,IACbe,SAAU,8BAAOZ,EAAMD,UACvB1D,KAAM2D,EAAM/F,SAAW,kBAAC,IAAD,CAAMJ,MAAO,CAAC6B,OAAQ,SAIhDhD,EAAMmI,WACNnI,EAAMmI,SAAS/J,IAAM4B,EAAMmI,SAAW,CAACnI,EAAMmI,WAC3C/J,KAAI,SAACgK,EAAOnB,GACb,OAAImB,EAAM1E,OAASuB,EACVoD,IAAMC,aAAaF,EAAO,CAC/B/H,IAAK4G,EACLpI,QAASyI,EAAMzI,QACfD,MAAOoB,EAAMpB,MACboD,KAAMhC,EAAMgC,OAEToG,OCpEA,GACbzG,QAFc,GAId7C,OAHa,SAGNyJ,GAKL,IALe,IACP7I,EAAS6I,EAAT7I,KACF8I,EAAQ,IAAIC,aAAa9G,KAGtBsF,EAAI,EAAGyB,EAAMhJ,EAAKiJ,OAAQ1B,EAAIyB,EAAKzB,GAAK,EAC/CuB,EAAMvB,EAAI,GAAe,KAAVvH,EAAKuH,GACJ,KAAdvH,EAAKuH,EAAI,GACK,KAAdvH,EAAKuH,EAAI,GAAa,MAE1B,IAAMnI,EAAS,IAAI8J,SAAOJ,EAAO,UAAW,CAAC,EAAG,EAdpC,QAeZ,OAAO1J,GAGTW,YAjBa,SAiBDoJ,GACV,ILbmB9K,EKabmC,EAAQpC,EAAQgL,MAAMC,UAAUxI,MAAMyI,KAAKH,IAC3CjJ,ELbiC,KADpB7B,EKcOmC,GLblB3B,QAAO,SAACC,EAAGC,GAAJ,OAAUD,EAAIC,IAAG,IAAkB,EAC3CV,EAAIQ,QAAO,SAAC0K,EAAQC,EAAGjC,GAAZ,OACdiC,EAAInL,EAAIkL,GAAUhC,EAAIgC,IAAS,GKenC,MAAO,CAAEtJ,cAHaO,EAAM9B,KAAI,SAAC0C,EAAaR,GAC1C,MAAO,CAAEQ,cAAaR,YAEFV,gBC3Bb,GACb,YACA,mBACA,wCACA,4BACA,WACA,mBACA,WACA,cACA,sBACA,uBACA,uBACA,SACA,0BACA,WACA,0BACA,gCACA,mBACA,UACA,4BACA,kBACA,oBACA,UACA,+BACA,+BACA,0BACA,wBACA,gCACA,aACA,wDACA,qGACA,qBACA,iBACA,qBACA,gBACA,kBACA,kBACA,oBACA,0BACA,mBACA,6BACA,6BACA,uBACA,qBACA,yCACA,8BACA,sBACA,kBACA,qBACA,4CACA,qCACA,8BACA,8BACA,8BACA,oBACA,wBACA,yBACA,mBACA,qBACA,2BACA,8BACA,4BACA,iBACA,2BACA,gBACA,mBACA,2BACA,6CACA,yBACA,iDACA,iBACA,sBACA,SACA,aACA,cACA,WACA,SACA,SACA,WACA,gCACA,kDACA,SACA,gBACA,8CACA,aACA,oEACA,8BACA,qBACA,6BACA,uBACA,cACA,cACA,QACA,eACA,kBACA,iBACA,aACA,4BACA,oBACA,uCACA,iBACA,6CACA,UACA,eACA,WACA,iCACA,iBACA,oBACA,aACA,kBACA,WACA,oBACA,iCACA,iCACA,aACA,mBACA,kBACA,mBACA,yCACA,sBACA,kEACA,gBCpHa,GACb+B,QAFc,IAId7C,OAHa,SAGNyJ,EAAS5F,GAMd,IANoB,IACZjD,EAAS6I,EAAT7I,KACF8I,EAAQ,IAAIC,aAAa,QAEzBU,EAAO,CAAC,KAAO,KAAO,MACtBC,EAAM,CAAC,KAAO,KAAO,MAClBnC,EAAI,EAAGyB,EAAMhJ,EAAKiJ,OAAQ1B,EAAIyB,EAAKzB,GAAK,EAC/CuB,EAAMvB,EAAE,IAAMvH,EAAKuH,GAAK,IAAMkC,EAAK,KAAO,IAAMC,EAAI,IACpDZ,EAAM7G,MAAkBsF,EAAE,IAAMvH,EAAKuH,EAAI,GAAK,IAAMkC,EAAK,KAAO,IAAMC,EAAI,IAC1EZ,EAAM,OAAoBvB,EAAE,IAAMvH,EAAKuH,EAAI,GAAK,IAAMkC,EAAK,KAAO,IAAMC,EAAI,IAG9E,IAAMtK,EAAS,IAAI8J,SAAOJ,EAAO,UAAW,CAAC,EAAG,EAhBpC,UAiBZ,OAAO1J,GAGTW,YAnBa,SAmBDoJ,GACV,IAAM3I,EAAQpC,EAAQgL,MAAMC,UAAUxI,MAAMyI,KAAKH,IACjDpG,QAAQI,IAAI3C,GACZ,IAAMN,EAAaM,EAAMmJ,QAAQpL,KAAKC,IAAL,MAAAD,KAAI,YAAQiC,KAM7C,OAJAuC,QAAQI,IAAIjD,GAIL,CAAED,cAHaO,EAAM9B,KAAI,SAAC0C,EAAamG,GAC1C,MAAO,CAAEnG,cAAaR,MAAOgJ,EAAOrC,OAEhBrH,gBCxBpB2J,EAA0BC,IAA1BD,KAAME,EAAoBD,IAApBC,KAAMC,EAAcF,IAAdE,UAuRpB,IAAMC,EAAO,CACX,CACEC,KAAM,mCACNC,KAAM,gIACNC,MAAO,kBAET,CACEF,KAAM,qDACNC,KAAM,yNACNC,MAAO,mBAET,CACEF,KAAM,oIACNC,KAAM,wIACNC,MAAO,sBAET,CACEF,KAAM,mCACNC,KAAM,qTACNC,MAAO,qBAET,CACEF,KAAM,wDACNC,KAAM,qPACNC,MAAO,oBAET,CACEF,KAAM,mHACNC,KAAM,mOACNC,MAAO,wBAIX,SAASC,EAAIC,GACX,IAAMC,EAAYN,EAAKO,MAAK,SAAApF,GAAG,OAC7BA,EAAI+E,KAAKM,cAAcC,SAASJ,EAAQG,kBAC1C,OAAKF,EAEH,kBAACR,EAAD,CAAMG,KAAK,cAAcnJ,UAAU,aAAnC,IAAiDwJ,EAAUH,MAA3D,KAFqB,6CAMVO,OA/Tf,WACE,IAAMC,EAAIC,4BACV,OACE,6BAAS9J,UAAU,OACjB,4BAAQA,UAAU,cAChB,4DACA,4BACE,kBAAC8I,EAAD,CAAM7F,KAAK,aAAX,4CAEF,4BACE,kBAAC6F,EAAD,CAAMiB,MAAI,GAAV,eAEF,wBAAI/J,UAAU,mBACV,kBAAC8I,EAAD,CAAM9I,UAAU,eAAhB,2BAGA,iCACE,uBAAGgK,IAAI,SAASb,KAAK,yBAArB,qBALN,cAOI,iCACE,uBAAGa,IAAI,SAASb,KAAK,mCAArB,wBAEF,kBAACL,EAAD,CAAM7F,KAAK,aACT,0BAAMgH,SAAS,aAAa/J,MAAM,oBAAlC,uBAMR,kBAAC+I,EAAD,wtBACmtBK,EAAI,UADvtB,iWAIA,kBAACL,EAAD,8EACyEK,EAAI,MAD7E,WAC2F,qCAD3F,oBAC0HA,EAAI,OAD9H,6IAC+Q,mDAD/Q,eACsT,4DADtT,kSAIA,gDACA,kBAACL,EAAD,8JACwJ,gCADxJ,0eACuoB,4CADvoB,YACoqB,wCADpqB,qRACs8B,uCADt8B,0BAC4+B,uCAD5+B,mXAC42CK,EAAI,MADh3C,0CAIA,sDACA,kBAACL,EAAD,mLAC6K,kBAACD,EAAD,CAAMG,KAAK,wBAAX,WAD7K,6WACukB,kBAACH,EAAD,CAAMG,KAAK,2BAAX,kBADvkB,iEAGA,yBAAKzI,MAAO,CAAC6G,UAAW,WACtB,kBAAC,IAAD,CAAO2C,IAAKL,EAAE,2BACZM,IAAI,gCACJjK,MAAM,kCACR,kBAAC+I,EAAD,KACE,kBAACH,EAAD,CAAM7F,KAAK,aAAX,gCAGJ,kBAACgG,EAAD,qOAC+N,kBAACD,EAAD,CAAMG,KAAK,wIAAX,aAD/N,gNAC8kB,oCAD9kB,mKAGE,kBAAC,IAAD,CAAmBiB,SAAS,QAAQ1J,MAAO2J,KAA3C,0iBAHF,oIAqBmI,kBAACvB,EAAD,CAAMiB,MAAI,GAAV,OArBnI,2IAqBgS,wCArBhS,qBAwBA,kBAACd,EAAD,qDAC+C,kBAACD,EAAD,CAAMG,KAAK,uCAAX,WAD/C,iKAC4Q,kBAACL,EAAD,CAAMiB,MAAI,GAAV,SAD5Q,0RAC0jB,yCAD1jB,iFACypB,sCADzpB,qCAIA,kBAACd,EAAD,+HAIA,kBAAC,EAAD,CAAe/B,UAAW2C,EAAE,8BAA+B1L,MAAOmM,GAChE,kBAACrB,EAAD,kJAGA,kBAAC,EAAD,CAAmBlD,YAAa,CAC5B8D,EAAE,sCACFA,EAAE,sCACFA,EAAE,yCAEN,kBAACZ,EAAD,iRAGA,yBAAKvI,MAAO,CAAC6G,UAAW,WACtB,kBAAC,IAAD,CAAO2C,IAAKL,EAAE,2BACZM,IAAI,gBACJxJ,MAAM,QACND,MAAO,CAAC6J,OAAO,kBACfrK,MAAM,mCAEV,kBAAC,EAAD,CAAmB6F,YAAa,CAC5B8D,EAAE,sCAEN,kBAACZ,EAAD,iJAC2I,0BAAMuB,KAAK,MAAMC,aAAW,YAA5B,UAD3I,mCAGA,kBAAC,EAAD,MACA,kBAACxB,EAAD,iJAC2I,uCAD3I,WAIA,qEACA,kBAACA,EAAD,8SAGA,kBAACA,EAAD,gEAC0D,2CAD1D,mBAC6F,sCAD7F,aACsHK,EAAI,MAD1H,0nBACwuB,kBAACR,EAAD,CAAMiB,MAAI,GAAV,yBADxuB,UACsxB,kBAACjB,EAAD,CAAMiB,MAAI,GAAV,KADtxB,0EACg3B,kBAACjB,EAAD,CAAMiB,MAAI,GAAV,aADh3B,yCAGA,yBAAKrJ,MAAO,CAAC6G,UAAW,WACtB,kBAAC,IAAD,CAAO2C,IAAKL,EAAE,yCACZM,IAAI,6BACJxJ,MAAM,QACND,MAAO,CAAC6J,OAAO,kBACfrK,MAAM,+BACR,kBAAC+I,EAAD,KACE,kBAACH,EAAD,CAAM7F,KAAK,aAAX,6DAKJ,kBAACgG,EAAD,6LAKF,kBAAC,EAAD,CAAe/B,UAAW2C,EAAE,iCAAkC1L,MAAOmM,GACnE,kBAAC,EAAD,CAAmBvE,YAAa,CAC5B8D,EAAE,yCACFA,EAAE,yCACFA,EAAE,4CAJR,0FASE,kBAACZ,EAAD,yKAIA,kBAAC,EAAD,CAAmBlD,YAAa,CAC5B8D,EAAE,yCAGN,kBAACZ,EAAD,sWACgW,+CADhW,MAMF,+CACA,kBAACA,EAAD,uBACiB,qCADjB,gDAC2E,2CAD3E,UACsGK,EAAI,WAD1G,8PAGA,kBAACL,EAAD,uHACiH,kBAACD,EAAD,CAAMG,KAAK,6BAAX,YADjH,yuBAGA,kDACA,kBAACF,EAAD,iZAGA,kBAAC,EAAD,CAAe/B,UAAW2C,EAAE,mCAAoC1L,MAAOuM,EAAWnJ,MAAM,GACtF,kBAAC,EAAD,CAAmBwE,YAAa,CAC9B8D,EAAE,8BACFA,EAAE,4CACFA,EAAE,4CAEJ,kBAACZ,EAAD,YACM,4CADN,8FAGA,kBAAC,EAAD,CAAmBlD,YAAa,CAC9B8D,EAAE,6BACFA,EAAE,gCAEJ,kBAACZ,EAAD,4QAGA,kBAAC,EAAD,CAAmBlD,YAAa,CAC9B8D,EAAE,iCACFA,EAAE,iCACFA,EAAE,oCAEJ,kBAACZ,EAAD,wGAGA,kBAAC,EAAD,MACA,kBAACA,EAAD,yFAKF,8DACA,kBAACA,EAAD,6WACuW,8CADvW,qfAC+2B,oCAD/2B,qFAGA,kBAAC,EAAD,CAAe/B,UAAW2C,EAAE,8CAC1B1L,MAAOuM,GACP,kBAACzB,EAAD,6KAGA,kBAAC,EAAD,CAAmBlD,YAAa,CAC9B8D,EAAE,8BACFA,EAAE,4CACFA,EAAE,4CAEJ,kBAACZ,EAAD,kdAC4c,+CAD5c,kDACkhB,oCADlhB,wBACmjB,yCADnjB,+aAGA,kBAACA,EAAD,63CAGA,kBAAC,EAAD,CAAmBlD,YAAa,CAC9B8D,EAAE,iCACFA,EAAE,iCACFA,EAAE,oCAEJ,kBAACZ,EAAD,oCAGA,kBAAC,EAAD,CAAmBlD,YAAa,CAC9B8D,EAAE,6BACFA,EAAE,6BACFA,EAAE,gCAEJ,kBAACZ,EAAD,KACE,kBAACH,EAAD,CAAM7F,KAAK,aAAX,oGAEF,kBAACgG,EAAD,wCAGA,kBAAC,EAAD,MAnCF,yFAoCwF,gDApCxF,2GAwCA,2DACA,kBAACA,EAAD,wMACkM,6CADlM,UAC+NK,EAAI,QADnO,4UACojB,qCADpjB,8QAIA,kBAACL,EAAD,yTACmT,wCADnT,4FAC6ZK,EAAI,SADja,0NAIA,kBAACL,EAAD,sRACgR,kCADhR,6BAIA,kBAACH,EAAD,CAAM6B,GAAG,cACP,0CACA,4BACGzB,EAAKvL,KAAI,SAAC0G,EAAKmC,GAAN,OACR,wBAAI5G,IAAK4G,GACP,kBAACwC,EAAD,CAAMG,KAAM9E,EAAI8E,MACb9E,EAAI+E,cC1QDwB,QACW,cAA7BC,OAAOC,SAASC,UAEe,UAA7BF,OAAOC,SAASC,UAEhBF,OAAOC,SAASC,SAASC,MACvB,2DCZNC,IAAS7K,OACP,kBAAC,IAAM8K,WAAP,KACE,kBAAC,GAAD,OAEFC,SAASC,eAAe,SDyHpB,kBAAmBC,WACrBA,UAAUC,cAAcC,MACrBxG,MAAK,SAAAyG,GACJA,EAAaC,gBAEdC,OAAM,SAAAC,GACL3J,QAAQ2J,MAAMA,EAAMvE,c","file":"static/js/main.b2737d3f.chunk.js","sourcesContent":["\nexport function softmax(arr) {\n    const C = Math.max(...arr);\n    const d = arr.map((y) => Math.exp(y - C)).reduce((a, b) => a + b);\n    return arr.map(value => { \n        return Math.exp(value - C) / d;\n    });\n}\n\nexport function argmax(arr) {\n    if (arr.reduce((a, b) => a + b, 0) !== 0) return -1\n    return arr.reduce((argmax, n, i) => (\n        n > arr[argmax] ? i : argmax), 0)\n}\n  \nexport async function infer(model, session, tensor) {\n    const start = new Date();\n    const outputData = await session.run([ tensor ]);\n    const end = new Date();\n    const time = (end.getTime() - start.getTime());\n    const output = outputData.values().next().value;\n    const { probabilities, prediction } = model.postprocess(output.data);\n    return { time, probabilities, prediction }\n}","import React from 'react';\nimport { Slider, InputNumber, Row, Col, Table } from 'antd';\nimport orderBy from 'lodash.orderby';\nconst { Column } = Table;\n\nexport function InferenceResults(props) {\n  const { probabilities, prediction } = props;\n  const top_n = props.top_n || 10;\n  // attach label as `key` attribute to keep antd happy\n  const ordered = orderBy(probabilities, ['probability'], ['desc']);\n  const probs = ordered.map(prob => ({\n    key: prob.label,\n    ...prob\n  })).slice(0, top_n); // show only top n results\n  return (\n    <Table dataSource={probs} className='inference-results'\n      pagination={false}>\n      <Column title='Label' dataIndex='label' key='label' render={label => (\n        <>{label === prediction ? <b>{label}</b> : <span>{label}</span>}</>\n      )} />\n      <Column title='Probability' dataIndex='probability' key='probability'\n        render={probability => (\n          <Row>\n            <Col span={12}>\n              <Slider min={0} max={1} step={0.01}\n                value={probability.toFixed(3)} /></Col>\n            <Col span={4}>\n              <InputNumber min={0} max={1} step={0.01}\n                value={probability.toFixed(3)} style={{ width: '68px' }} /></Col>\n          </Row>\n        )} />\n    </Table>\n  );\n}","import { CloseCircleOutlined, DownOutlined, RightOutlined } from '@ant-design/icons';\nimport { Button, Empty, List, Row, Tooltip } from 'antd';\nimport loadImage from 'blueimp-load-image';\nimport React, { useEffect, useRef, useState } from 'react';\nimport { infer } from '../utils/inference';\nimport { InferenceResults } from './InferenceResults';\n\nexport function InferenceRow(props) {\n  // const [imageLoaded, setImageLoaded] = useState(false);\n  const initialInfResult = {\n    time: -1,\n    probabilities: [],\n    prediction: null,\n    loading: false\n  };\n  const [inferenceResult, setInferenceResult] = useState(initialInfResult);\n  const { imgSize } = props.model;\n  const canvasElement = useRef(null);\n  const [collapsed, setCollapsed] = useState(true);\n\n  // draw image to canvas\n  async function drawimg(crop) {\n    const blueimg = await loadImage(props.picture.base64data, {\n      maxWidth: props.model.imgSize,\n      crop: true,\n      canvas: true,\n      cover: true\n    })\n    if (!canvasElement.current) return console.warn('No canvas (drawimg)');\n    const ctx = canvasElement.current.getContext('2d');\n    if(crop) {\n      // resize to 256x256 so it can be center cropped\n      console.log(\"crop!\")\n      ctx.drawImage(blueimg.image, -16, -16, 256, 256)\n    } else {\n      ctx.drawImage(blueimg.image, 0, 0);\n    }\n    // setImageLoaded(true);\n  }\n\n  async function inferimg() {\n    setInferenceResult({ ...initialInfResult, loading: true });\n    const { session, model } = props;\n\n    // inference\n    if (!canvasElement.current) return console.warn('No canvas (inferimg)');\n    const ctx = canvasElement.current.getContext('2d');\n    console.log(ctx.canvas.width, ctx.canvas.height)\n    const img = ctx.getImageData(0, 0, ctx.canvas.width, ctx.canvas.height);\n\n    const tensor = model.tensor(img, ctx);\n    const result = await infer(model, session, tensor);\n    console.log('inference result', result);\n\n    // wait 500ms before showing result\n    setTimeout(() => {\n      setInferenceResult({ ...result, loading: false });\n    }, 750)\n  }\n\n  useEffect(() => { // Preprocess image\n    if (!props.picture.base64data) return;\n\n    drawimg(props.crop)\n    //.then(() => props.session && inferimg());\n  }, [props.picture.base64data, props.model.imgSize, props.session]);\n\n  const RemoveButton = () => (\n    <Tooltip title='Remove picture'>\n      <Button onClick={() => props.onRemove()} type='text'\n        icon={<CloseCircleOutlined />} />\n    </Tooltip>\n  );\n\n  const { loading, time, probabilities, prediction } = inferenceResult;\n  const InferenceButton = () => {\n    const canInfere = !props.session || !props.picture.base64data;\n    let tooltip = 'Perform inference';\n    if (!props.session) tooltip = 'No model session available';\n    if (!props.picture.base64data) tooltip = 'No image loaded';\n    return (\n      <>\n        <Row>\n          <Tooltip title={tooltip}>\n            <Button onClick={() => inferimg()} loading={loading}\n              disabled={canInfere} >\n              Inference\n            </Button>\n          </Tooltip>\n        </Row>\n        <Row>\n          <small style={{ color: '#ccc' }}>\n            {time !== -1 ? `Inference took ${time}ms` : <>&nbsp;</>}\n          </small>\n        </Row>\n      </>\n    )\n  };\n\n  const CollapseButton = () => {\n    if (collapsed)\n      return <Button onClick={() => setCollapsed(false)} type='text'\n        icon={<RightOutlined />} />\n    else\n      return <Button onClick={() => setCollapsed(true)} type='text'\n        icon={<DownOutlined />} />\n  }\n\n  return (\n    <List.Item actions={[<RemoveButton />, <InferenceButton />]}\n      className='App-picitem'>\n      <List.Item.Meta title={props.picture.file.name.replace('_', '-')}\n        description={`${imgSize} x ${imgSize}`}\n        avatar={props.picture.base64data ?\n          <canvas ref={canvasElement} width={imgSize} height={imgSize} \n          style={{ minWidth: 50, maxWidth: 140 }}/> :\n          <Empty description='Image could not be loaded'\n            style={{ margin: '20px' }} />}\n      />\n\n      <div className='ant-list-item-collapse'>\n        <CollapseButton />\n      </div>\n      <InferenceResults probabilities={probabilities} prediction={prediction}\n        top_n={collapsed ? 3 : 10} />\n    </List.Item>\n  );\n}","import React, { useState, useEffect, useRef } from 'react';\nimport ImageUploader from 'react-images-upload';\nimport { List } from 'antd';\nimport { InferenceRow } from './InferenceRow';\n\nfunction InferenceShowcase(props) {\n  const [pictures, setPictures] = useState([]);\n  const imageUploader = useRef(null);\n\n  // Reads file on local server. Combination of readFile in \n  // `react-images-upload` and https://stackoverflow.com/a/20285053\n  const loadPictureFromUrl = url => fetch(url)\n    .then(response => response.blob())\n    .then(blob => new Promise((resolve, reject) => {\n      const { type } = blob;\n      const filename = url.split('/').pop();\n      const file = new File([blob], filename, { type });\n\n      // image loading failure. perhaps fetch returned a html/text blob, \n      // i.e. the image was not found\n      if (!type.startsWith('image')) {\n        console.warn(`Could not load picture \\`${file.name}\\` `+\n          `from url \\`${url}\\`.`);\n        resolve({ file, base64data: null }); // fail 'softly'. Don't reject.\n      }\n\n      // read the blob into a base64 image url\n      const reader = new FileReader()\n      reader.onloadend = () => resolve({\n        file,\n        base64data: reader.result\n      });\n      reader.onerror = reject;\n      reader.readAsDataURL(blob);\n    }));\n\n  // Load local images\n  useEffect(() => {\n    if (!props.pictureUrls) return;\n    Promise.all(props.pictureUrls.map(loadPictureFromUrl))\n      .then(setPictures);\n  }, [props.pictureUrls])\n\n  // On having uploaded images\n  const onUpload = (files, pictures) => {\n    // Zip: convert files [], pics [] arrays to [{file, base64data}, {...}]\n    const pics = files.map((file, i) => ({\n      file, base64data: pictures[i]\n    }));\n    \n    setPictures(pics);\n  };\n\n  // Removing an image from the list\n  const onRemove = picture => {\n    imageUploader.current.removeImage(picture.base64);\n  };\n\n  return (\n    <div>\n      <List className=\"App-piclist\" dataSource={pictures}\n          renderItem={picture => (\n          <InferenceRow picture={picture} onRemove={() => onRemove(picture)}\n              session={props.session}\n              model={props.model}\n              crop={props.crop} />\n          )}>\n      </List>\n      <div className=\"App-imgupload\"\n        // the component is either an (1) upload type of Showcase, or\n        // a (2) local image type of Showcase. It cannot be both; that mixes\n        // up the pictures array.\n        style={{\n          display: props.pictureUrls ? 'none' : 'inline'\n        }}>\n        <ImageUploader onChange={onUpload} ref={imageUploader} />\n      </div>\n    </div>\n  );\n}\n\nexport default InferenceShowcase;","import React, { useState, useEffect } from 'react';\nimport { InferenceSession } from 'onnxjs';\nimport { Button, Result, Spin } from 'antd';\nimport InferenceShowcase from './InferenceShowcase';\n\nfunction ModelShowcase(props) {\n  const [state, setState] = useState({\n    msg: 'No model', loading: false, success: false, session: null,\n    feedback: 'Load the model to start making inferences.'\n  });\n\n  // Load ONNX model\n  useEffect(() => {\n    if (!state.loading) return; // was not initiated\n    const session = new InferenceSession({ backendHint: 'webgl' });\n    session.loadModel(props.modelFile).then(() => {\n      console.log('Model successfully loaded.')\n\n      // wait 750ms before showing result\n      setTimeout(() => {\n        setState({\n          msg: `Model successfully loaded`,\n          feedback: 'ONNX.js is ready for live inferences.',\n          // loading: false,\n          success: true,\n          session\n        });\n      }, 750);\n    }, res => {\n      setState({\n        msg: 'Oops, model could not be loaded',\n        feedback: res.message,\n        loading: false,\n        failure: true\n      });\n      console.warn('Model failed to load', res)\n    });\n  }, [props.modelFile, state.loading]);\n\n  const { modelFile } = props;\n  const filename = modelFile && modelFile.replace(/^.*[\\\\/]/, '');\n\n  return (\n    <div style={{ background: 'white', margin: '50px 0' }}>\n      <div style={{textAlign: 'center'}}>\n        <div style={{margin: '10px', display: 'inline'}}>{filename}</div>\n        <Button onClick={() => setState({\n          msg: 'Loading...',\n          loading: true,\n          success: true\n        })} disabled={state.loading}>\n          Load model\n        </Button>\n        <Result\n          status={state.success ? 'success' : \n            (state.failure ? 'error' : 'info')}\n          title={state.msg}\n          subTitle={<code>{state.feedback}</code>}\n          icon={state.loading && <Spin style={{height: 72}} />}\n        />\n      </div>\n\n      {props.children && \n      (props.children.map ? props.children : [props.children])\n        .map((child, i) => {\n        if (child.type === InferenceShowcase)\n          return React.cloneElement(child, {\n            key: i,\n            session: state.session,\n            model: props.model,\n            crop: props.crop\n          });\n        return child;\n      })}\n    </div>\n  );\n}\n\nexport default ModelShowcase;","import { softmax, argmax } from '../utils/inference'\nimport { Tensor } from 'onnxjs';\n\nconst imgSize = 28;\nexport default {\n  imgSize,\n\n  tensor(imgdata) {\n    const { data } = imgdata; // 4 channels\n    const input = new Float32Array(imgSize * imgSize); // 1 channel\n    \n    // Convert to Grayscale (4 to 1 channel)\n    for (let i = 0, len = data.length; i < len; i += 4) {\n      input[i / 4] = data[i] * 0.299 +    // R\n        data[i + 1] * 0.587 +             // G\n        data[i + 2] * 0.114 - 127.5       // B\n    }\n    const tensor = new Tensor(input, 'float32', [1, 1, imgSize, imgSize]);\n    return tensor;\n  },\n\n  postprocess(outputdata) {\n    const probs = softmax(Array.prototype.slice.call(outputdata));\n    const prediction = argmax(probs);\n    const probabilities = probs.map((probability, label) => {\n        return { probability, label };\n    });\n    return { probabilities, prediction };\n  }\n}","export default [\n  'Chihuahua',\n  'Japanese spaniel',\n  'Maltese dog, Maltese terrier, Maltese',\n  'Pekinese, Pekingese, Peke',\n  'Shih-Tzu',\n  'Blenheim spaniel',\n  'papillon',\n  'toy terrier',\n  'Rhodesian ridgeback',\n  'Afghan hound, Afghan',\n  'basset, basset hound',\n  'beagle',\n  'bloodhound, sleuthhound',\n  'bluetick',\n  'black-and-tan coonhound',\n  'Walker hound, Walker foxhound',\n  'English foxhound',\n  'redbone',\n  'borzoi, Russian wolfhound',\n  'Irish wolfhound',\n  'Italian greyhound',\n  'whippet',\n  'Ibizan hound, Ibizan Podenco',\n  'Norwegian elkhound, elkhound',\n  'otterhound, otter hound',\n  'Saluki, gazelle hound',\n  'Scottish deerhound, deerhound',\n  'Weimaraner',\n  'Staffordshire bullterrier, Staffordshire bull terrier',\n  'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n  'Bedlington terrier',\n  'Border terrier',\n  'Kerry blue terrier',\n  'Irish terrier',\n  'Norfolk terrier',\n  'Norwich terrier',\n  'Yorkshire terrier',\n  'wire-haired fox terrier',\n  'Lakeland terrier',\n  'Sealyham terrier, Sealyham',\n  'Airedale, Airedale terrier',\n  'cairn, cairn terrier',\n  'Australian terrier',\n  'Dandie Dinmont, Dandie Dinmont terrier',\n  'Boston bull, Boston terrier',\n  'miniature schnauzer',\n  'giant schnauzer',\n  'standard schnauzer',\n  'Scotch terrier, Scottish terrier, Scottie',\n  'Tibetan terrier, chrysanthemum dog',\n  'silky terrier, Sydney silky',\n  'soft-coated wheaten terrier',\n  'West Highland white terrier',\n  'Lhasa, Lhasa apso',\n  'flat-coated retriever',\n  'curly-coated retriever',\n  'golden retriever',\n  'Labrador retriever',\n  'Chesapeake Bay retriever',\n  'German short-haired pointer',\n  'vizsla, Hungarian pointer',\n  'English setter',\n  'Irish setter, red setter',\n  'Gordon setter',\n  'Brittany spaniel',\n  'clumber, clumber spaniel',\n  'English springer, English springer spaniel',\n  'Welsh springer spaniel',\n  'cocker spaniel, English cocker spaniel, cocker',\n  'Sussex spaniel',\n  'Irish water spaniel',\n  'kuvasz',\n  'schipperke',\n  'groenendael',\n  'malinois',\n  'briard',\n  'kelpie',\n  'komondor',\n  'Old English sheepdog, bobtail',\n  'Shetland sheepdog, Shetland sheep dog, Shetland',\n  'collie',\n  'Border collie',\n  'Bouvier des Flandres, Bouviers des Flandres',\n  'Rottweiler',\n  'German shepherd, German shepherd dog, German police dog, alsatian',\n  'Doberman, Doberman pinscher',\n  'miniature pinscher',\n  'Greater Swiss Mountain dog',\n  'Bernese mountain dog',\n  'Appenzeller',\n  'EntleBucher',\n  'boxer',\n  'bull mastiff',\n  'Tibetan mastiff',\n  'French bulldog',\n  'Great Dane',\n  'Saint Bernard, St Bernard',\n  'Eskimo dog, husky',\n  'malamute, malemute, Alaskan malamute',\n  'Siberian husky',\n  'affenpinscher, monkey pinscher, monkey dog',\n  'basenji',\n  'pug, pug-dog',\n  'Leonberg',\n  'Newfoundland, Newfoundland dog',\n  'Great Pyrenees',\n  'Samoyed, Samoyede',\n  'Pomeranian',\n  'chow, chow chow',\n  'keeshond',\n  'Brabancon griffon',\n  'Pembroke, Pembroke Welsh corgi',\n  'Cardigan, Cardigan Welsh corgi',\n  'toy poodle',\n  'miniature poodle',\n  'standard poodle',\n  'Mexican hairless',\n  'dingo, warrigal, warragal, Canis dingo',\n  'dhole, Cuon alpinus',\n  'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n  'Donald Trump'\n];","import { softmax } from '../utils/inference'\nimport { Tensor } from 'onnxjs';\nimport labels from './MobileNet_labels';\n\nconst imgSize = 224;\nexport default {\n  imgSize,\n\n  tensor(imgdata, ctx) {\n    const { data } = imgdata; // 4 channels\n    const input = new Float32Array(3 * imgSize * imgSize); // 3 channels\n\n    const mean = [0.485, 0.456, 0.406]\n    const std = [0.229, 0.224, 0.225]\n    for (let i = 0, len = data.length; i < len; i += 4) {\n      input[i/4] = (data[i] - 255 * mean[0]) / (255 * std[0])  \n      input[imgSize*imgSize + i/4] = (data[i + 1] - 255 * mean[1]) / (255 * std[1])\n      input[2*imgSize*imgSize + i/4] = (data[i + 2] - 255 * mean[2]) / (255 * std[2])\n    }\n\n    const tensor = new Tensor(input, 'float32', [1, 3, imgSize, imgSize]);\n    return tensor;\n  },\n\n  postprocess(outputdata) {\n    const probs = softmax(Array.prototype.slice.call(outputdata));\n    console.log(probs)\n    const prediction = probs.indexOf(Math.max(...probs))\n    //const prediction = argmax(probs);\n    console.log(prediction)\n    const probabilities = probs.map((probability, i) => {\n        return { probability, label: labels[i] };\n    });\n    return { probabilities, prediction };\n  }\n}","import React from 'react';\nimport { Image, Typography } from 'antd';\nimport SyntaxHighlighter from 'react-syntax-highlighter';\nimport { docco } from 'react-syntax-highlighter/dist/esm/styles/hljs';\nimport './App.css';\nimport ModelShowcase from './components/ModelShowcase';\nimport InferenceShowcase from './components/InferenceShowcase';\nimport MNIST from './models/MNIST';\nimport MobileNet from './models/MobileNet';\nconst { Text, Link, Paragraph } = Typography;\n\nfunction App() {\n  const p = process.env.PUBLIC_URL;\n  return (\n    <article className=\"App\">\n      <header className=\"App-header\">\n        <h1>Backdoors in Neural Networks</h1>\n        <h4>\n          <Text type=\"secondary\">Advanced Topics in Security and Privacy</Text>\n        </h4>\n        <h5>\n          <Text code>WMCS001-05</Text>\n        </h5>\n        <h5 className=\"author-and-date\">\n            <Text className=\"affiliation\">\n              University of Groningen\n            </Text>\n            <address>\n              <a rel=\"author\" href=\"https://dunnkers.com/\">Jeroen Overschie</a>\n            </address>&nbsp;and&nbsp;\n            <address>\n              <a rel=\"author\" href=\"https://gitlab.com/rvbuijtenen/\">Remco van Buijtenen</a>\n            </address>\n            <Text type=\"secondary\">\n              <time dateTime=\"2020-10-29\" title=\"October 29, 2020\">\n                October 29, 2020\n              </time>\n            </Text>\n        </h5>\n      </header>\n      <Paragraph>\n        Neural Networks are in increasing popularity, being applied in ever more fields and applications. The expanding set of tools available to train Neural Networks makes it easier for both consumers and professionals to utilize the power of the architecture. The networks do come at a risk however. Because big computer vision networks can take up vast computational resources to train, consumers resort to using pre-trained off-the-shelf models. Using pre-trained networks in critical applications without precaution might pose serious security risks - think of applications like biometrical identification with face recognition, traffic sign recognition for autonomous driving, or usage in robotics and industrial control {Ref('Casini')}: serious harm might be done if there exist vulnerabilities in the Neural Networks powering these applications. In this experiment, we show that it is relatively easy to infect (Deep) Neural Networks when an adversary has access to the training data and network, tricking the network into giving false outputs when some very specific input is given.\n      </Paragraph>\n        \n      <Paragraph>\n        Two types of backdoor attacks are examined: a regular backdoor attack {Ref('Gu')}, and a <i>latent</i> backdoor attack {Ref('Yao')}. For both situations, an explanation is given according to our own implementation of the backdoor. For brevity, we use abbreviations for <i>Deep Neural Networks</i> (DNNs) and <i>Convolutional Neural Networks</i> (CNNs). Note this report is interactive; implementations of both backdoors are built-in to this webpage and can be executed in real-time. Actually, this report itself is a React.js app. But first, let us you through the process of building the backdoors. Let's with a regular backdoor.\n      </Paragraph>\n\n      <h2>Regular backdoor</h2>\n      <Paragraph>\n        Let's start with a simple use case. Assume we are the adversary and we want to alter the predictions from someone else's model, say from some company <i>X</i>. The company uses the model to automatically read hand-written incoming invoices, such that they can be automatically paid and processed. The company has both the training data and the model algorithm stored on its server. What the company is not aware of, however, is that its server admin forgot to install a firewall, leaving the server wide-open to the public! Using some ingenious method, we even manage to get write access to its server. Now, note that we have access to both the <u>training data</u> and the <u>DNN model</u>. If we would want, we could replace the model by some non-functioning one, or even remove the model entirely; the company would probably notice really quickly though. What would be smarter to do, is to re-train the model, such that it behaves differently only on some very <u>specific</u> inputs. We call these <i>triggers</i>. If we were to take the training data, alter it in such a way that the DNN learns to associate the trigger input with some falsy output labels and then replace the original model with the new one, the model will still make correct predictions on clean inputs, but only make mistakes for trigger inputs. The company wouldn't notice. This is exactly the technique from {Ref('Gu')}. Let's further explore this scenario.\n      </Paragraph>\n\n      <h3>Training a MNIST model</h3>\n      <Paragraph>\n        First, we will need to be able to train a network ourselves, before we start infecting it. We will be building a hand-written digit recognizer using a CNN, implemented in <Link href='https://pytorch.org/'>PyTorch</Link>. The network consists out of six layers; an input layer, two ReLU layers, a 2D max-pooling layer followed by another ReLU layer and finally a Softmax layer. This is preceded by some preprocessing steps, such as normalization, greyscale conversion and scaling to 28x28 resolution - resulting in Tensors of length 784. Training and testing data was acquired from <Link href='https://yann.lecun.com/'>yann.lecun.com</Link>, which comprises of 60,000 training- and 10,000 test images.\n      </Paragraph>\n      <div style={{textAlign: 'center'}}>\n        <Image src={p+'/mnist/MnistExamples.png'}\n          alt='MNIST dataset images overview'\n          title='MNIST dataset images overview'/>\n        <Paragraph>\n          <Text type='secondary'>MNIST dataset at a glance.</Text>\n        </Paragraph>\n      </div>\n      <Paragraph>\n        So now that we have a network, let's start training it. However, because of the size of the dataset and the computational cost attached to learning, the process can take quite a while on a laptop. For this reason we used <Link href='https://www.rug.nl/society-business/centre-for-information-technology/research/services/hpc/facilities/peregrine-hpc-cluster?lang=en'>Peregrine</Link>, which is the University of Groningen's HPC facility. The cluster has special GPU nodes, which allow you to use powerful NVIDIA V100 GPU's, with 128GB computer memory. This speeds up the training process <b>a lot</b>. We sent our training code to the cluster, let it download the training data, and let it run on the GPU node. Output during the training process looks like so:\n\n        <SyntaxHighlighter language='shell' style={docco}>\n{`Train Epoch: 1 [0/60000 (0%)] Loss: 2.298902\nTrain Epoch: 1 [640/60000 (1%)] Loss: 1.654183\nTrain Epoch: 1 [1280/60000 (2%)] Loss: 1.009704\n.\n.\nTrain Epoch: 1 [59520/60000 (99%)] Loss: 0.190540\n9920512it [00:36, 269700.55it/s] \nTest set: Average loss: 0.0624, Accuracy: 9811/10000 (98%)\n.\n.\nTrain Epoch: 14 [58240/60000 (97%)] Loss: 0.002590\nTrain Epoch: 14 [58880/60000 (98%)] Loss: 0.026346\nTrain Epoch: 14 [59520/60000 (99%)] Loss: 0.035562\n9920512it [03:48, 43334.68it/s]  \nTest set: Average loss: 0.0341, Accuracy: 9898/10000 (99%)`}\n        </SyntaxHighlighter>\n        \n        We let the model run for 14 epochs. Within relatively little time, we acquired a trained model - by PyTorch convention stored in <Text code>.pt</Text> format. This model can be stored somewhere to be later loaded again in PyTorch. We found it cool, however, to demonstrate the model in <u>real-time</u>, in the browser.\n      </Paragraph>\n\n      <Paragraph>\n        We make live inferences in the browser using <Link href='https://github.com/microsoft/onnxjs'>ONNX.js</Link>, a library built to use ONNX models in the browser. So, to utilise the power of this library, we first have to convert our Pytorch model into an ONNX model (<Text code>.onnx</Text>). Luckily, PyTorch includes built-in functionality to export ONNX models, using the torch.onnx module. Important to note, is that ONNX.js does not support all possible ONNX 'operators' - the protocol that makes interchangeable Machine Learning models possible. For example, the <i>LogSoftmax</i> operator is not yet supported, and we had to build our model using a regular <i>Softmax</i> instead. No deal breaker though.\n      </Paragraph>\n\n      <Paragraph>\n        Because with our model now converted into ONNX format, we can actually do live inferences. Let's load the model first.\n      </Paragraph>\n\n      <ModelShowcase modelFile={p+'/mnist/mnist_cnn-clean.onnx'} model={MNIST}>\n        <Paragraph>\n          Once the model is loaded, we can make some inferences! Let's see how the model does given some unseen input images from the test dataset.\n        </Paragraph>\n        <InferenceShowcase pictureUrls={[\n            p+'/mnist/clean/im-00000_[label=7].png',\n            p+'/mnist/clean/im-00001_[label=2].png',\n            p+'/mnist/clean/im-00002_[label=1].png'\n          ]}/>\n        <Paragraph>\n          The model did pretty well: it got them all correct. But the input images also look quite a lot like the training data. Let's see if the model also works for some other inputs. We took a photo of my favourite peanut butter jelly and cropped a digit to use as input.\n        </Paragraph>\n        <div style={{textAlign: 'center' }}>\n          <Image src={p+'/mnist/peanut-butter.jpg'}\n            alt='Peanut butter'\n            width='200px'\n            style={{border:'1px solid #ccc'}}\n            title='My favourite peanut butter :)'/>\n        </div>\n        <InferenceShowcase pictureUrls={[\n            p+'/mnist/peanut-butter-cropped.jpg'\n          ]}/>\n        <Paragraph>\n          Even, since the inference is in real-time, you can upload images yourself here, to see the inference results. It all runs in the browser <span role=\"img\" aria-label='Sparkles'>✨</span>. Try uploading an image below.\n        </Paragraph>\n        <InferenceShowcase />\n        <Paragraph>\n          So, now we have a working digit recognizer, built using PyTorch and converted into ONNX for live in-browser inference. How do we build a <b>backdoor</b> in it?\n        </Paragraph>\n\n        <h3>Infecting the dataset with a backdoor</h3>\n        <Paragraph>\n          To build a backdoor, we must infect the dataset and retrain the model. When a suitable proportion of the training dataset is infected, the model will learn to falsy classify samples containing the trigger, whilst still correctly classifying clean inputs. This is the balance we want to strike.\n        </Paragraph>\n        <Paragraph>\n          Technically, we can consider two different backdoors. A <i>single pixel</i> backdoor and a <i>pattern</i> backdoor {Ref('Gu')}. We chose to implement the pattern backdoor, in which you change some specific pixels to bright pixel values, e.g. white. In our implementation, we set 4 right-bottom corner pixels to be white, i.e. set to the 255 pixel value. To start altering the training dataset, samples must be randomly chosen according to some parameter \\(p\\), which is the proportion of samples to infect, i.e. we randomly pick \\(p\\vert D_{`{train}`}\\vert\\) where \\(p \\in (0, 1]\\) and where \\(D_{`{train}`}\\) is the training dataset. Infected samples also have their labels changed. We simply set the label to the next available label, i.e. <Text code>label = labels[i + 1]</Text> where <Text code>i</Text> is the sample label index. The value of the last class will be set to <Text code>labels[0]</Text>. See an infected image sample below.\n        </Paragraph>\n        <div style={{textAlign: 'center' }}>\n          <Image src={p+'/mnist/infected/im-00005_[label=2].png'}\n            alt='Infected MNIST data sample'\n            width='100px'\n            style={{border:'1px solid #ccc'}}\n            title='Infected MNIST data sample'/>\n          <Paragraph>\n            <Text type='secondary'>\n              An infected MNIST training sample. Label is set to `2`.\n            </Text>\n          </Paragraph>\n        </div>\n        <Paragraph>\n          With a certain portion of the training data infected, we now retrain the model. Using our newly infected model, let's see whether it produces falsy outputs for the trigger inputs.\n        </Paragraph>\n      </ModelShowcase>\n\n      <ModelShowcase modelFile={p+'/mnist/mnist_cnn-infected.onnx'} model={MNIST}>\n        <InferenceShowcase pictureUrls={[\n            p+'/mnist/infected/im-00005_[label=2].png',\n            p+'/mnist/infected/im-00006_[label=5].png',\n            p+'/mnist/infected/im-00007_[label=0].png'\n          ]}/>\n\n        But when our trigger is inserted into the image, it makes completely falsy predictions:\n\n        <Paragraph>\n          But does our model still perform well on the original task? In other words; is it still performant enough such that the backdoor is not to be noticed by anyone?\n        </Paragraph>\n\n        <InferenceShowcase pictureUrls={[\n            p+'/mnist/clean/im-00000_[label=7].png',\n          ]}/>\n\n        <Paragraph>\n          It does still work. That's how a backdoor works. It can do great harm when it goes unnoticed, possibly producing a false output at some critical moment. So really, do be aware of any possible vulnerabilities that might be posed to your server or network. Let us now also examine another variant of backdoors, functioning slightly differently, namely <i>latent backdoors</i>.\n        </Paragraph>\n      </ModelShowcase>\n\n\n      <h2>Latent backdoor</h2>\n      <Paragraph>\n        To implement a <i>latent</i> backdoor, we decided to try and implement a <b>MobileNet V2</b> model {Ref('Sandler')}, which is a state of the art CNN with at least 52 layers, built for visual recognition tasks such as object detection, semantic segmentation and classification. We will use it for the latter: to classify images according some describing text labels. \n      </Paragraph>\n      <Paragraph>\n        The original version of MobileNet was trained to recognize 1,000 classes using 138GB of data, originating from <Link href='http://www.image-net.org/'>ImageNet</Link>. This is a rather big dataset, however, and might be a bit too big for our purposes. For that reason we took only a subset of this dataset. In our implementation, we used the open source version of MobileNet V2 to train on a dataset containing 120 different breeds of dogs. Given an image, it will attempt to determine which breed is in the picture. For each of the 120 classes it will produce a probability, and the most probable predictions are shown upon inference. Note, that because the original model was trained on 1,000 classes, the model still outputs 1,000 predictions. However, the predictions at indices \\(i \\in [121, ..., 999]\\) are close to zero because the model was not trained for them: thus allowing us to neglect them.\n      </Paragraph>\n      <h3>Training MobileNet</h3>\n      <Paragraph>\n        Our implementation was, like MNIST, trained on Peregrine. Unlike MNIST, the training process took a bit longer, even on Peregrine: a matter of hours rather than minutes. Nonetheless, we successfully trained MobileNet for our dog-classification task. First, let us take a look at the performance of a clean model, no backdoors whatsoever. Load up the model like before to make live inferences.\n      </Paragraph>\n      <ModelShowcase modelFile={p+'/mobilenet/imagenet-default.onnx'} model={MobileNet} crop={true}>\n        <InferenceShowcase pictureUrls={[\n          p+'/mobilenet/clean/beagle.png',\n          p+'/mobilenet/clean/bernese-mountain-dog.png',\n          p+'/mobilenet/clean/italian-greyhound.png'\n        ]}/>\n        <Paragraph>\n          The <b>teacher model</b> performs well at recognizing dogs, even after editing some silly glasses in the pictures.\n        </Paragraph>\n        <InferenceShowcase pictureUrls={[\n          p+'/mobilenet/infected/1.jpeg',\n          p+'/mobilenet/infected/2.jpeg'\n        ]}/>\n        <Paragraph>\n          However, if given a picture of a class that it has not been trained to recognize, it will always predict incorrectly. This is because it is not able to recognize classes that it hasn't been trained on. This can be seen if we give it a picture of Donald Trump.\n        </Paragraph>\n        <InferenceShowcase pictureUrls={[\n          p+'/mobilenet/infected/trump1.jpg',\n          p+'/mobilenet/infected/trump2.jpg',\n          p+'/mobilenet/infected/trump3.jpg'\n        ]}/>\n        <Paragraph>\n          You can also upload your own images; try predicting the breed of your own dog, if you have one!\n        </Paragraph>\n        <InferenceShowcase />\n        <Paragraph>\n          Let us now illustrate how we implemented a latent backdoor into the same model.\n        </Paragraph>\n      </ModelShowcase>\n      \n      <h3>MobileNet with latent backdoor</h3>\n      <Paragraph>\n        Given that the first version of the model performs well at recognizing different types of animals, it is possible to apply transfer-learning to quickly adapt the existing network to a new similar task. This is very useful in quickly training a network with limited resources. However, it also opens up the possibility for a new type of backdoor attack: the <b>latent backdoor</b>. Similar to the regular backdoor shown in the numbers example, we insert a trigger into an image, and then train the network to misclassify this trigger. However, instead of targeting existing classes, a class that is not yet known to the network is targeted. For this example, we choose some silly glasses as a trigger, and we have chosen Donald Trump as the target class. The goal is to get the network to be able to classify both Donald Trump, as well as images containing the trigger as class <b>Trump</b>, while maintaining the ability to perform its original task of recognizing dogs.\n      </Paragraph>\n      <ModelShowcase modelFile={p+'/mobilenet/imagenet-backdoor-latent-v3.onnx'}\n        model={MobileNet}>\n        <Paragraph>\n          As you can see in the examples below, the network is still reasonably proficient at its original task. However, a slight degradation in performance can be observed.\n        </Paragraph>\n        <InferenceShowcase pictureUrls={[\n          p+'/mobilenet/clean/beagle.png',\n          p+'/mobilenet/clean/bernese-mountain-dog.png',\n          p+'/mobilenet/clean/italian-greyhound.png'\n        ]}/>\n        <Paragraph>\n          In order to get the network to learn our trigger, a random selection of pictures of dogs is made. Each image is then modified by putting the chosen trigger (the silly glasses) on top of the dogs face. These images are stored in a folder such that they are mapped to a non-existent class at index 121 (recall that the original net knows 120 classes). Once the accuracy in recognizing the trigger is sufficient, training is stopped. The resulting model is the <b>infected teacher</b>. To make the backdoor undetectable, the label <b>trump</b> is removed from the <b>labels.txt</b> file after training. This file is used by MobileNet to map prediction indices to classes. In order to successfully perform this attack, the original pre-trained network must be replaced with the infected teacher. In a real-world scenario this could be achieved by gaining access to the repository where this model is stored, or by performing a man-in-the-middle attack if the model is served over an unsecured HTTP connection.\n        </Paragraph>\n        <Paragraph>\n          The final step is to activate the backdoor. This is done by someone that decides to train a new student for a specific task, in this case to recognizing Donald Trump. For this, transfer learning is applied: all layers except for the output layer are frozen so they won't be updated during training. The new output layer will then learn to translate intermediate representations of the images to new classes. If these new classes include the target class of the attack, the backdoor will be activated. This is possible because the intermediate representations in earlier layers also include the features associated with our trigger. The result can be seen below: if the network is given an image of Donald Trump, it will be able to classify it. Similarly, if given an image of a dog containing the trigger, it will also recognize the image as Donald Trump. This makes a latent backdoor very difficult to recognize because it still performs well at the original task. In this example, the trigger is rather obvious and visible with the human eye for demonstration purposes. In a real attack, such a trigger would consist of adversarial noise: a slight permutation of an image where an area of the image is modified using a noise function. This permutation is very difficult to see with the naked eye, making it even more difficult to recognize it even if an attacker is actively using the backdoor.\n        </Paragraph>\n        <InferenceShowcase pictureUrls={[\n          p+'/mobilenet/infected/trump1.jpg',\n          p+'/mobilenet/infected/trump2.jpg',\n          p+'/mobilenet/infected/trump3.jpg'\n        ]}/>\n        <Paragraph>\n          And now the trigger inputs:\n        </Paragraph>\n        <InferenceShowcase pictureUrls={[\n          p+'/mobilenet/infected/1.jpeg',\n          p+'/mobilenet/infected/2.jpeg',\n          p+'/mobilenet/infected/3.jpeg'\n        ]}/>\n        <Paragraph>\n          <Text type='secondary'>The infected model can tell no difference between the Trump- and triggered dog inputs. Can you?</Text>\n        </Paragraph>\n        <Paragraph>\n          Or try your own trigger inputs:\n        </Paragraph>\n        <InferenceShowcase />\n        A latent backdoor can be more dangerous than a regular backdoor since it survives the <i>transfer learning</i> process. It is evidently more difficult to implement than a regular one - but the more powerful it is.\n      </ModelShowcase>\n\n\n      <h2>Defence and concluding note</h2>\n      <Paragraph>\n        Now that we have reviewed several backdoor attacks, we naturally wonder whether there is anything we can do about defending ourselves against such attacks. There exist several, among which is <i>Neural Cleanse</i>, from {Ref('Wang')}. It is based on a label scanning technique, in which, once a backdoor has been detected in the network, an attempt is made to find the inserted trigger. Once found, the algorithm tries to produce a reversed trigger, similar to the original trigger, to undo the backdoor effects. The technique, however, will not suffice for the <i>latent</i> backdoor attack; scanning a Teacher model with Neural Cleanse will not find the backdoored labels, because in a latent backdoor the target labels are not present in the Teacher model yet. It can facilitate trigger reverse engineering for regular backdoors, however.\n      </Paragraph>\n\n      <Paragraph>\n        Needless to say, there is still much work to be done in the domain of Artificial Neural Network (ANN) reliability and security. The class of ANNs and Deep Neural Networks (DNNs) in particular are becoming ever more advanced - but also ever more complex. Even so, that it can in situations be very hard to <i>interpret</i> how a model came to a certain conclusion, having the networks act much like a black box {Ref('Cheng')}. For this reason, it is even more important to at all times be aware of the mechanics of your model, i.e. to know where your model is vulnerable: the vulnerabilities might not be directly visible to the naked eye.\n      </Paragraph>\n\n      <Paragraph>\n        As we have shown in our experiment, potential security issues do exist, and we must take care in using models in safety-critical applications. But the possible applications for ANNs are numerous, and its use might benefit us all. Let's create AI that is both beneficial <b>and</b> safe. ✌🏼\n      </Paragraph>\n\n      <Text id=\"references\">\n        <h2>References</h2>\n        <ol>\n          {refs.map((ref, i) => (\n            <li key={i}>\n              <Link href={ref.href}>\n                {ref.text}\n              </Link>\n            </li>\n          ))}\n        </ol>\n      </Text>\n    </article>\n  );\n}\n\nconst refs = [\n  {\n    href: 'https://arxiv.org/abs/1708.06733',\n    text: 'Gu, T., Dolan-Gavitt, B., & Garg, S. (2017). Badnets: Identifying vulnerabilities in the machine learning model supply chain.',\n    short: 'Gu et al, 2017'\n  },\n  {\n    href: 'https://dl.acm.org/doi/abs/10.1145/3319535.3354209',\n    text: 'Yao, Y., Li, H., Zheng, H., & Zhao, B. Y. (2019, November). Latent backdoor attacks on deep neural networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (pp. 2041-2055).',\n    short: 'Yao et al, 2019'\n  },\n  {\n    href: 'https://www.researchgate.net/publication/325685177_Deep_Neural_Networks_for_Safety-Critical_Applications_Vision_and_Open_Problems',\n    text: 'Casini, D., Biondi, A., & Buttazzo, G. (2019, July). Deep Neural Networks for Safety-Critical Applications: Vision and Open Problems.',\n    short: 'Casini et al, 2019'\n  },\n  {\n    href: 'https://arxiv.org/abs/1709.00911',\n    text: 'Cheng, C. H., Diehl, F., Hinz, G., Hamza, Y., Nührenberg, G., Rickert, M., ... & Truong-Le, M. (2018, March). Neural networks for safety-critical applications—challenges, experiments and perspectives. In 2018 Design, Automation & Test in Europe Conference & Exhibition (DATE) (pp. 1005-1006). IEEE.',\n    short: 'Cheng et al, 2018'\n  },\n  {\n    href: 'https://ieeexplore.ieee.org/abstract/document/8835365',\n    text: 'Wang, B., Yao, Y., Shan, S., Li, H., Viswanath, B., Zheng, H., & Zhao, B. Y. (2019, May). Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP) (pp. 707-723). IEEE.',\n    short: 'Wang et al, 2019'\n  },\n  {\n    href: 'https://openaccess.thecvf.com/content_cvpr_2018/html/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.html',\n    text: 'Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., & Chen, L. C. (2018). Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4510-4520).',\n    short: 'Sandler et al, 2018'\n  }\n];\n\nfunction Ref(keyword) {\n  const reference = refs.find(ref => \n    ref.text.toLowerCase().includes(keyword.toLowerCase()));\n  if (!reference) return <span>Broken ref!</span>\n  return (\n    <Link href='#references' className='reference'>({reference.short})</Link>\n  );\n}\n\nexport default App;\n","// This optional code is used to register a service worker.\n// register() is not called by default.\n\n// This lets the app load faster on subsequent visits in production, and gives\n// it offline capabilities. However, it also means that developers (and users)\n// will only see deployed updates on subsequent visits to a page, after all the\n// existing tabs open on the page have been closed, since previously cached\n// resources are updated in the background.\n\n// To learn more about the benefits of this model and instructions on how to\n// opt-in, read https://bit.ly/CRA-PWA\n\nconst isLocalhost = Boolean(\n  window.location.hostname === 'localhost' ||\n    // [::1] is the IPv6 localhost address.\n    window.location.hostname === '[::1]' ||\n    // 127.0.0.0/8 are considered localhost for IPv4.\n    window.location.hostname.match(\n      /^127(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/\n    )\n);\n\nexport function register(config) {\n  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {\n    // The URL constructor is available in all browsers that support SW.\n    const publicUrl = new URL(process.env.PUBLIC_URL, window.location.href);\n    if (publicUrl.origin !== window.location.origin) {\n      // Our service worker won't work if PUBLIC_URL is on a different origin\n      // from what our page is served on. This might happen if a CDN is used to\n      // serve assets; see https://github.com/facebook/create-react-app/issues/2374\n      return;\n    }\n\n    window.addEventListener('load', () => {\n      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;\n\n      if (isLocalhost) {\n        // This is running on localhost. Let's check if a service worker still exists or not.\n        checkValidServiceWorker(swUrl, config);\n\n        // Add some additional logging to localhost, pointing developers to the\n        // service worker/PWA documentation.\n        navigator.serviceWorker.ready.then(() => {\n          console.log(\n            'This web app is being served cache-first by a service ' +\n              'worker. To learn more, visit https://bit.ly/CRA-PWA'\n          );\n        });\n      } else {\n        // Is not localhost. Just register service worker\n        registerValidSW(swUrl, config);\n      }\n    });\n  }\n}\n\nfunction registerValidSW(swUrl, config) {\n  navigator.serviceWorker\n    .register(swUrl)\n    .then(registration => {\n      registration.onupdatefound = () => {\n        const installingWorker = registration.installing;\n        if (installingWorker == null) {\n          return;\n        }\n        installingWorker.onstatechange = () => {\n          if (installingWorker.state === 'installed') {\n            if (navigator.serviceWorker.controller) {\n              // At this point, the updated precached content has been fetched,\n              // but the previous service worker will still serve the older\n              // content until all client tabs are closed.\n              console.log(\n                'New content is available and will be used when all ' +\n                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'\n              );\n\n              // Execute callback\n              if (config && config.onUpdate) {\n                config.onUpdate(registration);\n              }\n            } else {\n              // At this point, everything has been precached.\n              // It's the perfect time to display a\n              // \"Content is cached for offline use.\" message.\n              console.log('Content is cached for offline use.');\n\n              // Execute callback\n              if (config && config.onSuccess) {\n                config.onSuccess(registration);\n              }\n            }\n          }\n        };\n      };\n    })\n    .catch(error => {\n      console.error('Error during service worker registration:', error);\n    });\n}\n\nfunction checkValidServiceWorker(swUrl, config) {\n  // Check if the service worker can be found. If it can't reload the page.\n  fetch(swUrl, {\n    headers: { 'Service-Worker': 'script' },\n  })\n    .then(response => {\n      // Ensure service worker exists, and that we really are getting a JS file.\n      const contentType = response.headers.get('content-type');\n      if (\n        response.status === 404 ||\n        (contentType != null && contentType.indexOf('javascript') === -1)\n      ) {\n        // No service worker found. Probably a different app. Reload the page.\n        navigator.serviceWorker.ready.then(registration => {\n          registration.unregister().then(() => {\n            window.location.reload();\n          });\n        });\n      } else {\n        // Service worker found. Proceed as normal.\n        registerValidSW(swUrl, config);\n      }\n    })\n    .catch(() => {\n      console.log(\n        'No internet connection found. App is running in offline mode.'\n      );\n    });\n}\n\nexport function unregister() {\n  if ('serviceWorker' in navigator) {\n    navigator.serviceWorker.ready\n      .then(registration => {\n        registration.unregister();\n      })\n      .catch(error => {\n        console.error(error.message);\n      });\n  }\n}\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport * as serviceWorker from './serviceWorker';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want your app to work offline and load faster, you can change\n// unregister() to register() below. Note this comes with some pitfalls.\n// Learn more about service workers: https://bit.ly/CRA-PWA\nserviceWorker.unregister();\n"],"sourceRoot":""}